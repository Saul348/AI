{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.applications.imagenet_utils import preprocess_input"
      ],
      "metadata": {
        "id": "-IO1a3c-WtVh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "# Cambiar por el folder donde se encuentren los datos.\n",
        "drive.mount('/content/drive')\n",
        "folder = '/content/drive/MyDrive/chest_xray'\n",
        "# Elegimos el tamaño del Batch, el tamaño de imagen y los pasos por epoca \n",
        "batch_size = 16\n",
        "image_shape = (128, 128)\n",
        "STEPS_PER_EPOCH = 20\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKp6EDV053PS",
        "outputId": "1656e9a1-95e8-40ea-bdfc-994972cb1078"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cargar la Red\n",
        "\n",
        "En este caso para las capas convolucionales usamos la RedNet50 para obtener ciertos rasgos de la imagen"
      ],
      "metadata": {
        "id": "C7Tm3LmS36Z7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.applications import ResNet50, EfficientNetB3\n",
        "\n",
        "retnet50 = ResNet50(\n",
        "  weights='imagenet',\n",
        "  #Fijamos el include top en falso para no cargar capas que no usaremos. \n",
        "  include_top=False,\n",
        "  input_shape=(*image_shape, 3)\n",
        ")"
      ],
      "metadata": {
        "id": "G_wJmcVMX6ap",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbad35ff-4f68-4763-f5c1-e832871a681a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "94765736/94765736 [==============================] - 4s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generador de Imagenes"
      ],
      "metadata": {
        "id": "SLAFekkEGvju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "datagen = ImageDataGenerator(\n",
        "  rescale=1/255,\n",
        "  preprocessing_function=preprocess_input)\n",
        "\n",
        "#Generamos los directorios con los subdirectorios que definen las clases \n",
        "train_generator = datagen.flow_from_directory(\n",
        "    folder + '/train',\n",
        "    target_size = image_shape,\n",
        "    batch_size  = batch_size,\n",
        "    shuffle = True,\n",
        "    class_mode  = 'binary'\n",
        ")\n",
        "\n",
        "test_generator = datagen.flow_from_directory(\n",
        "    folder + '/test',\n",
        "    target_size = image_shape,\n",
        "    batch_size  = batch_size,\n",
        "    class_mode  = 'binary'\n",
        ")\n",
        "\n",
        "eval_generator = datagen.flow_from_directory(\n",
        "    folder + '/val',\n",
        "    target_size = image_shape,\n",
        "    batch_size  = batch_size,\n",
        "    class_mode  = 'binary'\n",
        ")\n",
        "#Notemos que la base datos no está balanceada "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogaab5LpS6FK",
        "outputId": "f3acba71-4cee-452d-f9a1-9040b20b7fbc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 5217 images belonging to 2 classes.\n",
            "Found 624 images belonging to 2 classes.\n",
            "Found 16 images belonging to 2 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generar red neuronal\n",
        "\n",
        "Creamos las siguientes capas densas de la red congelamos las capas convolucionales de la rednet50 (Backbone\n",
        ")"
      ],
      "metadata": {
        "id": "UDiYSaXv4S-G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import models\n",
        "from keras import layers\n",
        "import tensorflow as tf\n",
        "#Creamos las capas densas \n",
        "model = models.Sequential()\n",
        "model.add(retnet50) \n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(32, activation='relu'))\n",
        "model.add(layers.Dense(1, activation='sigmoid'))\n",
        "#Congelamos las capas convolucionales \n",
        "retnet50.trainable = False\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iy-TStWQnxW0",
        "outputId": "083e56e5-939a-443d-c2b3-79d4b496cf24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " resnet50 (Functional)       (None, 4, 4, 2048)        23587712  \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 32768)             0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 128)               4194432   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 64)                8256      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 32)                2080      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 27,792,513\n",
            "Trainable params: 4,204,801\n",
            "Non-trainable params: 23,587,712\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Compilamos el modelo con la función de perdida de entropía cruzada  y el optimizador adam\n",
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['acc']\n",
        ")\n",
        "#Guardar el mejor modelo \n",
        "model_route = '/content/drive/MyDrive/best_model.h5'\n",
        "save_model = tf.keras.callbacks.ModelCheckpoint(\n",
        "  filepath=model_route,\n",
        "  save_weights_only=True,\n",
        "  verbose=0, \n",
        "  save_best_only=True, \n",
        "  monitor='val_loss', \n",
        "  mode='min'\n",
        ")"
      ],
      "metadata": {
        "id": "01Re7NpKp5QZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Dado que la base de datos no esta balanceada, calculamos unos pesos especiales \n",
        "pesos = {\n",
        "  0 : train_generator.classes.shape[0] / np.count_nonzero(train_generator.classes==0), \n",
        "  1 : train_generator.classes.shape[0] / np.count_nonzero(train_generator.classes)\n",
        "}\n",
        "\n",
        "print(pesos)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bRcJaPtrDoYl",
        "outputId": "5db5eab3-b63e-44f1-b638-29ebb36b21bc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{0: 3.8874813710879286, 1: 1.3463225806451613}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(\n",
        "  train_generator,\n",
        "  steps_per_epoch = STEPS_PER_EPOCH, \n",
        "  epochs          = 25,\n",
        "  validation_data = eval_generator,\n",
        "  validation_steps= 1,\n",
        "  class_weight = pesos,   \n",
        "  verbose         = 2,\n",
        "  callbacks = [save_model]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzvr22vsqPF2",
        "outputId": "b3dcd846-76c0-4cf8-b812-f7dccb48cb78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Descongelamos el Backbone\n"
      ],
      "metadata": {
        "id": "a4tdy8k345ws"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Descongelamos y entrenamos las ultimas 25 capas de la red convolucional \n",
        "retnet50.trainable = True\n",
        "layer_names = [layer.name for layer in retnet50.layers]\n",
        "for layer in retnet50.layers:\n",
        "    if layer.name in layer_names[-25:]:\n",
        "        layer.trainable = True\n",
        "    else:\n",
        "        layer.trainable = False\n",
        "        \n",
        "retnet50.summary()"
      ],
      "metadata": {
        "id": "VAaoG7Ri3k4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compilamos el modelo con la función de perdida de entropía cruzada  y el optimizador adam\n",
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['acc']\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "  train_generator,\n",
        "  steps_per_epoch = STEPS_PER_EPOCH, \n",
        "  epochs          = 25,\n",
        "  validation_data = eval_generator,\n",
        "  validation_steps= 1,\n",
        "  class_weight = pesos,\n",
        "  verbose         = 2,\n",
        "  callbacks = [save_model]\n",
        ")"
      ],
      "metadata": {
        "id": "DuNkfnei4xPL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Full tuning \n",
        "\n"
      ],
      "metadata": {
        "id": "Yr55o2d55RU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Descongelamos todas las capas de la red neuronal.\n",
        "for layer in retnet50.layers:\n",
        "  layer.trainable = True\n",
        "      \n",
        "retnet50.summary()"
      ],
      "metadata": {
        "id": "MmjNP0HT6vCJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Compilamos el modelo con la función de perdida de entropía crusada  y el optimizador adam\n",
        "model.compile(\n",
        "  loss='binary_crossentropy',\n",
        "  optimizer='adam',\n",
        "  metrics=['acc']\n",
        ")\n",
        "\n",
        "model.fit(\n",
        "  train_generator,\n",
        "  steps_per_epoch = STEPS_PER_EPOCH, \n",
        "  epochs          = 20,\n",
        "  validation_data = eval_generator,\n",
        "  validation_steps= 1,\n",
        "  class_weight = pesos,\n",
        "  verbose         = 2,\n",
        "  callbacks = [save_model]\n",
        ")"
      ],
      "metadata": {
        "id": "OCDsY5ZT67bc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('ModeloTa4.h5')"
      ],
      "metadata": {
        "id": "R1OtTpA9ntQj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Resultados"
      ],
      "metadata": {
        "id": "_Ai3kCP_5Xor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_generator)\n"
      ],
      "metadata": {
        "id": "yDzrdtIe_Vmi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score, recall_score, precision_score\n",
        "\n",
        "y_true_labels = test_generator.classes\n",
        "y_pred_labels = np.round(model.predict(test_generator))\n",
        "\n",
        "print(f\"\"\"\n",
        "Test results\n",
        "Accuracy : {accuracy_score(y_true_labels, y_pred_labels)}\n",
        "Recall : {recall_score(y_true_labels, y_pred_labels)}\n",
        "Specificity : {recall_score(y_true_labels, y_pred_labels, pos_label = 0)}\n",
        "Precision : {precision_score(y_true_labels, y_pred_labels)}\n",
        "\"\"\")"
      ],
      "metadata": {
        "id": "37WsjX7XEAgv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "C = confusion_matrix(y_true_labels, y_pred_labels)\n",
        "\n",
        "f, ax = plt.subplots(figsize=(11, 9))\n",
        "sns.set()\n",
        "cmap = sns.diverging_palette(220, 10, as_cmap=True)\n",
        "ax = sns.heatmap(C, cmap=cmap, square=True, linewidths=.5)\n",
        "ax.set_title('Matriz de Confusión')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "XvhbauyFOGiq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc\n",
        "\n",
        "fpr, tpr, _ = roc_curve(y_true_labels, y_pred_labels)\n",
        "roc_auc = auc(fpr, tpr)\n",
        "\n",
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(\n",
        "    fpr,\n",
        "    tpr,\n",
        "    color=\"darkorange\",\n",
        "    lw=lw,\n",
        "    label=\"ROC curve (area = %0.2f)\" % roc_auc,\n",
        ")\n",
        "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver operating characteristic example\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "jB7-qbJnmdBt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Se intentaron algunas variantes para mejorar el modelo pero todo termina de forma similar. Me parece que la red esta prediciendo muchas veces una clase. Se intento guardar el mejor modelo para el validation encontrado, dejar que corra, aumentar el numero de neuronas en la densa, diferentes optimizadores y todas los intentos tuvieron resultados similares que no fueron muy buenos. Me parece  que el desbalance de los datos afecta bastante a la red neuronal aun cuando se ajustaron los pesos."
      ],
      "metadata": {
        "id": "fkzNx9125fYJ"
      }
    }
  ]
}