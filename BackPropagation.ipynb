{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KVkAfsu4S00T"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# Cargamos la base de Datos Iris \n",
        "iris = load_iris()\n",
        "X = iris.data\n",
        "y = iris.target\n",
        "\n",
        "# Codificamos las etiquetas como one-hot, es decir, creamos una matriz \n",
        "# de ceros y unos, donde los 1's se agregan en la columna con el mismo valor de\n",
        "#la etiqueta y las demas entradas son cero.\n",
        "y_onehot = np.zeros((len(y), 3))\n",
        "y_onehot[np.arange(len(y)), y] = 1\n",
        "# Separamos los datos de entrenamiento y de prueba considerando que ahora \n",
        "# que las etiquetas ahora se toman de la matriz one hot\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y_onehot, test_size=0.2)\n",
        "\n",
        "#Separamos el tamaño del input, damos el numero de capaz ocultas y el tamaño de \n",
        "#salida.\n",
        "input_size = X_train.shape[1]\n",
        "hidden_size = 50\n",
        "output_size = 3\n",
        "\n",
        "# Inicializamos los pesos de manera random para la capa oculta y la capa de sali\n",
        "# da. Además inicializamos los baias como un vector de ceros.\n",
        "W1 = .5*np.random.randn(input_size, hidden_size)\n",
        "b1 = np.zeros(hidden_size)\n",
        "W2 = .5*np.random.randn(hidden_size, output_size)\n",
        "b2 = np.zeros(output_size)\n",
        "\n",
        "# Definimos las funciones de activación y la función de perdida \n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def softmax(x):\n",
        "    exp_x = np.exp(x)\n",
        "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)\n",
        "\n",
        "# Definimos la función de pérdida (Entropía Cruzada)\n",
        "def binary_cross_entropy(y_pred, y_true):\n",
        "    epsilon = 1e-12\n",
        "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon)  # Clippear los valores para evitar log(0)\n",
        "    n = y_pred.shape[0]\n",
        "    loss = -(1/n) * np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    return loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Definimos la función de predicción \n",
        "def pred(X,W1,b1,W2,b2):\n",
        "  hidden_layer = np.dot(X, W1) + b1\n",
        "  hidden_layer_act = sigmoid(hidden_layer)\n",
        "  ouput_layer = np.dot(hidden_layer_act, W2) + b2\n",
        "  output_layer_act = softmax(ouput_layer)\n",
        "  return output_layer_act\n",
        "# Implementamos el algoritmo descenso Gradiente Descendiente Estocástico\n",
        "def stochastic_gradient_descent(X, y, W1, b1, W2, b2, learning_rate):\n",
        "    num_samples = X.shape[0]\n",
        "    for i in range(num_samples):\n",
        "        # Implementamos la propagación hacia delante \n",
        "        hidden_layer = np.dot(X, W1) + b1\n",
        "        hidden_layer_act = sigmoid(hidden_layer)\n",
        "        ouput_layer = np.dot(hidden_layer_act, W2) + b2\n",
        "        output_layer_act = softmax(ouput_layer)\n",
        "        # Calculamos la pérdida y el gradiente con la Entropía Cruzada\n",
        "        loss = binary_cross_entropy(output_layer_act, y)\n",
        "        Grad_Out_La = output_layer_act - y\n",
        "        Grad_W2 = np.dot(hidden_layer_act.T, Grad_Out_La)\n",
        "        Grad_b2 = np.sum(Grad_Out_La, axis= 0)\n",
        "        Grad_Out_La_Act = np.dot(Grad_Out_La, W2.T)\n",
        "        Grad_Hid_La = Grad_Out_La_Act * (hidden_layer > 0)\n",
        "        Grad_W1 = np.dot(X.T, Grad_Hid_La)\n",
        "        Grad_b1 = np.sum(Grad_Hid_La, axis= 0)\n",
        "        # Propagamos hacia atrás para actualizar los pesos y los sesgos\n",
        "        W1 -= learning_rate * Grad_W1\n",
        "        b1 -= learning_rate * Grad_b1\n",
        "        W2 -= learning_rate * Grad_W2 \n",
        "        b2 -= learning_rate * Grad_b2\n",
        "    return W1, b1, W2, b2\n"
      ],
      "metadata": {
        "id": "qwQusWI9TNjd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "learning_rate= 0.1\n",
        "num_epochs= 300\n",
        "batch_size= 40\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    # Randomizamos los datos de entrenamiento\n",
        "    indices = np.random.permutation(X_train.shape[0])\n",
        "    X_train = X_train[indices]\n",
        "    y_train = y_train[indices]\n",
        "\n",
        "    # Dividimos  los datos de entrenamiento en lotes\n",
        "    for i in range(0, X_train.shape[0], batch_size):\n",
        "        X_batch = X_train[i:i+batch_size]\n",
        "        y_batch = y_train[i:i+batch_size]\n",
        "        W1, b1, W2, b2 = stochastic_gradient_descent(X_batch, y_batch, W1, b1, W2, b2, learning_rate)\n",
        "    y_pred= pred(X_train,W1,b1,W2,b2)\n",
        "    y_pred_ = np.argmax(y_pred, axis=1)\n",
        "    y_train_ = np.argmax(y_train, axis=1)\n",
        "    print(\"Accuracy en la epoca \", epoch, \" : \", accuracy_score(y_train_, y_pred_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LLnqfogVWNMh",
        "outputId": "bcbe0086-ac6e-49c7-b12d-1885bbb5ce34"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy en la epoca  0  :  0.975\n",
            "Accuracy en la epoca  1  :  0.975\n",
            "Accuracy en la epoca  2  :  0.975\n",
            "Accuracy en la epoca  3  :  0.975\n",
            "Accuracy en la epoca  4  :  0.9833333333333333\n",
            "Accuracy en la epoca  5  :  0.975\n",
            "Accuracy en la epoca  6  :  0.9833333333333333\n",
            "Accuracy en la epoca  7  :  0.9666666666666667\n",
            "Accuracy en la epoca  8  :  0.975\n",
            "Accuracy en la epoca  9  :  0.9666666666666667\n",
            "Accuracy en la epoca  10  :  0.975\n",
            "Accuracy en la epoca  11  :  0.9666666666666667\n",
            "Accuracy en la epoca  12  :  0.9583333333333334\n",
            "Accuracy en la epoca  13  :  0.9666666666666667\n",
            "Accuracy en la epoca  14  :  0.9666666666666667\n",
            "Accuracy en la epoca  15  :  0.9666666666666667\n",
            "Accuracy en la epoca  16  :  0.975\n",
            "Accuracy en la epoca  17  :  0.975\n",
            "Accuracy en la epoca  18  :  0.975\n",
            "Accuracy en la epoca  19  :  0.975\n",
            "Accuracy en la epoca  20  :  0.975\n",
            "Accuracy en la epoca  21  :  0.975\n",
            "Accuracy en la epoca  22  :  0.975\n",
            "Accuracy en la epoca  23  :  0.9666666666666667\n",
            "Accuracy en la epoca  24  :  0.9833333333333333\n",
            "Accuracy en la epoca  25  :  0.975\n",
            "Accuracy en la epoca  26  :  0.9583333333333334\n",
            "Accuracy en la epoca  27  :  0.975\n",
            "Accuracy en la epoca  28  :  0.975\n",
            "Accuracy en la epoca  29  :  0.9583333333333334\n",
            "Accuracy en la epoca  30  :  0.9666666666666667\n",
            "Accuracy en la epoca  31  :  0.975\n",
            "Accuracy en la epoca  32  :  0.975\n",
            "Accuracy en la epoca  33  :  0.975\n",
            "Accuracy en la epoca  34  :  0.9833333333333333\n",
            "Accuracy en la epoca  35  :  0.9583333333333334\n",
            "Accuracy en la epoca  36  :  0.975\n",
            "Accuracy en la epoca  37  :  0.975\n",
            "Accuracy en la epoca  38  :  0.975\n",
            "Accuracy en la epoca  39  :  0.975\n",
            "Accuracy en la epoca  40  :  0.975\n",
            "Accuracy en la epoca  41  :  0.975\n",
            "Accuracy en la epoca  42  :  0.975\n",
            "Accuracy en la epoca  43  :  0.975\n",
            "Accuracy en la epoca  44  :  0.975\n",
            "Accuracy en la epoca  45  :  0.975\n",
            "Accuracy en la epoca  46  :  0.975\n",
            "Accuracy en la epoca  47  :  0.9833333333333333\n",
            "Accuracy en la epoca  48  :  0.975\n",
            "Accuracy en la epoca  49  :  0.9583333333333334\n",
            "Accuracy en la epoca  50  :  0.975\n",
            "Accuracy en la epoca  51  :  0.975\n",
            "Accuracy en la epoca  52  :  0.975\n",
            "Accuracy en la epoca  53  :  0.975\n",
            "Accuracy en la epoca  54  :  0.975\n",
            "Accuracy en la epoca  55  :  0.975\n",
            "Accuracy en la epoca  56  :  0.975\n",
            "Accuracy en la epoca  57  :  0.9666666666666667\n",
            "Accuracy en la epoca  58  :  0.9833333333333333\n",
            "Accuracy en la epoca  59  :  0.975\n",
            "Accuracy en la epoca  60  :  0.975\n",
            "Accuracy en la epoca  61  :  0.975\n",
            "Accuracy en la epoca  62  :  0.975\n",
            "Accuracy en la epoca  63  :  0.975\n",
            "Accuracy en la epoca  64  :  0.975\n",
            "Accuracy en la epoca  65  :  0.9833333333333333\n",
            "Accuracy en la epoca  66  :  0.975\n",
            "Accuracy en la epoca  67  :  0.975\n",
            "Accuracy en la epoca  68  :  0.975\n",
            "Accuracy en la epoca  69  :  0.975\n",
            "Accuracy en la epoca  70  :  0.975\n",
            "Accuracy en la epoca  71  :  0.975\n",
            "Accuracy en la epoca  72  :  0.975\n",
            "Accuracy en la epoca  73  :  0.975\n",
            "Accuracy en la epoca  74  :  0.9833333333333333\n",
            "Accuracy en la epoca  75  :  0.975\n",
            "Accuracy en la epoca  76  :  0.975\n",
            "Accuracy en la epoca  77  :  0.975\n",
            "Accuracy en la epoca  78  :  0.975\n",
            "Accuracy en la epoca  79  :  0.975\n",
            "Accuracy en la epoca  80  :  0.9833333333333333\n",
            "Accuracy en la epoca  81  :  0.975\n",
            "Accuracy en la epoca  82  :  0.975\n",
            "Accuracy en la epoca  83  :  0.9833333333333333\n",
            "Accuracy en la epoca  84  :  0.95\n",
            "Accuracy en la epoca  85  :  0.975\n",
            "Accuracy en la epoca  86  :  0.9583333333333334\n",
            "Accuracy en la epoca  87  :  0.975\n",
            "Accuracy en la epoca  88  :  0.9833333333333333\n",
            "Accuracy en la epoca  89  :  0.975\n",
            "Accuracy en la epoca  90  :  0.9833333333333333\n",
            "Accuracy en la epoca  91  :  0.975\n",
            "Accuracy en la epoca  92  :  0.975\n",
            "Accuracy en la epoca  93  :  0.975\n",
            "Accuracy en la epoca  94  :  0.975\n",
            "Accuracy en la epoca  95  :  0.975\n",
            "Accuracy en la epoca  96  :  0.975\n",
            "Accuracy en la epoca  97  :  0.975\n",
            "Accuracy en la epoca  98  :  0.975\n",
            "Accuracy en la epoca  99  :  0.9833333333333333\n",
            "Accuracy en la epoca  100  :  0.9666666666666667\n",
            "Accuracy en la epoca  101  :  0.975\n",
            "Accuracy en la epoca  102  :  0.975\n",
            "Accuracy en la epoca  103  :  0.975\n",
            "Accuracy en la epoca  104  :  0.9833333333333333\n",
            "Accuracy en la epoca  105  :  0.975\n",
            "Accuracy en la epoca  106  :  0.975\n",
            "Accuracy en la epoca  107  :  0.975\n",
            "Accuracy en la epoca  108  :  0.9833333333333333\n",
            "Accuracy en la epoca  109  :  0.975\n",
            "Accuracy en la epoca  110  :  0.9833333333333333\n",
            "Accuracy en la epoca  111  :  0.975\n",
            "Accuracy en la epoca  112  :  0.975\n",
            "Accuracy en la epoca  113  :  0.975\n",
            "Accuracy en la epoca  114  :  0.975\n",
            "Accuracy en la epoca  115  :  0.975\n",
            "Accuracy en la epoca  116  :  0.975\n",
            "Accuracy en la epoca  117  :  0.975\n",
            "Accuracy en la epoca  118  :  0.975\n",
            "Accuracy en la epoca  119  :  0.975\n",
            "Accuracy en la epoca  120  :  0.975\n",
            "Accuracy en la epoca  121  :  0.9833333333333333\n",
            "Accuracy en la epoca  122  :  0.975\n",
            "Accuracy en la epoca  123  :  0.9833333333333333\n",
            "Accuracy en la epoca  124  :  0.975\n",
            "Accuracy en la epoca  125  :  0.975\n",
            "Accuracy en la epoca  126  :  0.975\n",
            "Accuracy en la epoca  127  :  0.975\n",
            "Accuracy en la epoca  128  :  0.9833333333333333\n",
            "Accuracy en la epoca  129  :  0.975\n",
            "Accuracy en la epoca  130  :  0.975\n",
            "Accuracy en la epoca  131  :  0.9833333333333333\n",
            "Accuracy en la epoca  132  :  0.975\n",
            "Accuracy en la epoca  133  :  0.975\n",
            "Accuracy en la epoca  134  :  0.9833333333333333\n",
            "Accuracy en la epoca  135  :  0.975\n",
            "Accuracy en la epoca  136  :  0.975\n",
            "Accuracy en la epoca  137  :  0.975\n",
            "Accuracy en la epoca  138  :  0.9833333333333333\n",
            "Accuracy en la epoca  139  :  0.9833333333333333\n",
            "Accuracy en la epoca  140  :  0.975\n",
            "Accuracy en la epoca  141  :  0.975\n",
            "Accuracy en la epoca  142  :  0.975\n",
            "Accuracy en la epoca  143  :  0.9833333333333333\n",
            "Accuracy en la epoca  144  :  0.975\n",
            "Accuracy en la epoca  145  :  0.9833333333333333\n",
            "Accuracy en la epoca  146  :  0.975\n",
            "Accuracy en la epoca  147  :  0.9833333333333333\n",
            "Accuracy en la epoca  148  :  0.9833333333333333\n",
            "Accuracy en la epoca  149  :  0.9583333333333334\n",
            "Accuracy en la epoca  150  :  0.9666666666666667\n",
            "Accuracy en la epoca  151  :  0.9833333333333333\n",
            "Accuracy en la epoca  152  :  0.9833333333333333\n",
            "Accuracy en la epoca  153  :  0.975\n",
            "Accuracy en la epoca  154  :  0.975\n",
            "Accuracy en la epoca  155  :  0.975\n",
            "Accuracy en la epoca  156  :  0.975\n",
            "Accuracy en la epoca  157  :  0.975\n",
            "Accuracy en la epoca  158  :  0.9666666666666667\n",
            "Accuracy en la epoca  159  :  0.9833333333333333\n",
            "Accuracy en la epoca  160  :  0.975\n",
            "Accuracy en la epoca  161  :  0.975\n",
            "Accuracy en la epoca  162  :  0.975\n",
            "Accuracy en la epoca  163  :  0.975\n",
            "Accuracy en la epoca  164  :  0.9833333333333333\n",
            "Accuracy en la epoca  165  :  0.975\n",
            "Accuracy en la epoca  166  :  0.9833333333333333\n",
            "Accuracy en la epoca  167  :  0.9833333333333333\n",
            "Accuracy en la epoca  168  :  0.975\n",
            "Accuracy en la epoca  169  :  0.975\n",
            "Accuracy en la epoca  170  :  0.975\n",
            "Accuracy en la epoca  171  :  0.975\n",
            "Accuracy en la epoca  172  :  0.9833333333333333\n",
            "Accuracy en la epoca  173  :  0.975\n",
            "Accuracy en la epoca  174  :  0.975\n",
            "Accuracy en la epoca  175  :  0.9416666666666667\n",
            "Accuracy en la epoca  176  :  0.975\n",
            "Accuracy en la epoca  177  :  0.975\n",
            "Accuracy en la epoca  178  :  0.975\n",
            "Accuracy en la epoca  179  :  0.9833333333333333\n",
            "Accuracy en la epoca  180  :  0.975\n",
            "Accuracy en la epoca  181  :  0.975\n",
            "Accuracy en la epoca  182  :  0.975\n",
            "Accuracy en la epoca  183  :  0.9833333333333333\n",
            "Accuracy en la epoca  184  :  0.975\n",
            "Accuracy en la epoca  185  :  0.9833333333333333\n",
            "Accuracy en la epoca  186  :  0.975\n",
            "Accuracy en la epoca  187  :  0.975\n",
            "Accuracy en la epoca  188  :  0.975\n",
            "Accuracy en la epoca  189  :  0.975\n",
            "Accuracy en la epoca  190  :  0.95\n",
            "Accuracy en la epoca  191  :  0.9833333333333333\n",
            "Accuracy en la epoca  192  :  0.975\n",
            "Accuracy en la epoca  193  :  0.9833333333333333\n",
            "Accuracy en la epoca  194  :  0.975\n",
            "Accuracy en la epoca  195  :  0.975\n",
            "Accuracy en la epoca  196  :  0.975\n",
            "Accuracy en la epoca  197  :  0.9833333333333333\n",
            "Accuracy en la epoca  198  :  0.975\n",
            "Accuracy en la epoca  199  :  0.9833333333333333\n",
            "Accuracy en la epoca  200  :  0.9416666666666667\n",
            "Accuracy en la epoca  201  :  0.9833333333333333\n",
            "Accuracy en la epoca  202  :  0.975\n",
            "Accuracy en la epoca  203  :  0.975\n",
            "Accuracy en la epoca  204  :  0.9833333333333333\n",
            "Accuracy en la epoca  205  :  0.9833333333333333\n",
            "Accuracy en la epoca  206  :  0.9833333333333333\n",
            "Accuracy en la epoca  207  :  0.975\n",
            "Accuracy en la epoca  208  :  0.975\n",
            "Accuracy en la epoca  209  :  0.975\n",
            "Accuracy en la epoca  210  :  0.9833333333333333\n",
            "Accuracy en la epoca  211  :  0.975\n",
            "Accuracy en la epoca  212  :  0.9833333333333333\n",
            "Accuracy en la epoca  213  :  0.975\n",
            "Accuracy en la epoca  214  :  0.975\n",
            "Accuracy en la epoca  215  :  0.9666666666666667\n",
            "Accuracy en la epoca  216  :  0.975\n",
            "Accuracy en la epoca  217  :  0.9833333333333333\n",
            "Accuracy en la epoca  218  :  0.975\n",
            "Accuracy en la epoca  219  :  0.9833333333333333\n",
            "Accuracy en la epoca  220  :  0.975\n",
            "Accuracy en la epoca  221  :  0.975\n",
            "Accuracy en la epoca  222  :  0.9833333333333333\n",
            "Accuracy en la epoca  223  :  0.9833333333333333\n",
            "Accuracy en la epoca  224  :  0.975\n",
            "Accuracy en la epoca  225  :  0.975\n",
            "Accuracy en la epoca  226  :  0.975\n",
            "Accuracy en la epoca  227  :  0.975\n",
            "Accuracy en la epoca  228  :  0.975\n",
            "Accuracy en la epoca  229  :  0.9833333333333333\n",
            "Accuracy en la epoca  230  :  0.975\n",
            "Accuracy en la epoca  231  :  0.975\n",
            "Accuracy en la epoca  232  :  0.975\n",
            "Accuracy en la epoca  233  :  0.975\n",
            "Accuracy en la epoca  234  :  0.975\n",
            "Accuracy en la epoca  235  :  0.9833333333333333\n",
            "Accuracy en la epoca  236  :  0.975\n",
            "Accuracy en la epoca  237  :  0.975\n",
            "Accuracy en la epoca  238  :  0.975\n",
            "Accuracy en la epoca  239  :  0.975\n",
            "Accuracy en la epoca  240  :  0.975\n",
            "Accuracy en la epoca  241  :  0.975\n",
            "Accuracy en la epoca  242  :  0.9833333333333333\n",
            "Accuracy en la epoca  243  :  0.975\n",
            "Accuracy en la epoca  244  :  0.9833333333333333\n",
            "Accuracy en la epoca  245  :  0.975\n",
            "Accuracy en la epoca  246  :  0.9833333333333333\n",
            "Accuracy en la epoca  247  :  0.9833333333333333\n",
            "Accuracy en la epoca  248  :  0.975\n",
            "Accuracy en la epoca  249  :  0.975\n",
            "Accuracy en la epoca  250  :  0.975\n",
            "Accuracy en la epoca  251  :  0.9833333333333333\n",
            "Accuracy en la epoca  252  :  0.975\n",
            "Accuracy en la epoca  253  :  0.975\n",
            "Accuracy en la epoca  254  :  0.9833333333333333\n",
            "Accuracy en la epoca  255  :  0.9833333333333333\n",
            "Accuracy en la epoca  256  :  0.9833333333333333\n",
            "Accuracy en la epoca  257  :  0.975\n",
            "Accuracy en la epoca  258  :  0.975\n",
            "Accuracy en la epoca  259  :  0.9833333333333333\n",
            "Accuracy en la epoca  260  :  0.975\n",
            "Accuracy en la epoca  261  :  0.975\n",
            "Accuracy en la epoca  262  :  0.975\n",
            "Accuracy en la epoca  263  :  0.9833333333333333\n",
            "Accuracy en la epoca  264  :  0.975\n",
            "Accuracy en la epoca  265  :  0.975\n",
            "Accuracy en la epoca  266  :  0.975\n",
            "Accuracy en la epoca  267  :  0.975\n",
            "Accuracy en la epoca  268  :  0.975\n",
            "Accuracy en la epoca  269  :  0.975\n",
            "Accuracy en la epoca  270  :  0.9833333333333333\n",
            "Accuracy en la epoca  271  :  0.975\n",
            "Accuracy en la epoca  272  :  0.975\n",
            "Accuracy en la epoca  273  :  0.975\n",
            "Accuracy en la epoca  274  :  0.975\n",
            "Accuracy en la epoca  275  :  0.9833333333333333\n",
            "Accuracy en la epoca  276  :  0.975\n",
            "Accuracy en la epoca  277  :  0.975\n",
            "Accuracy en la epoca  278  :  0.9833333333333333\n",
            "Accuracy en la epoca  279  :  0.975\n",
            "Accuracy en la epoca  280  :  0.975\n",
            "Accuracy en la epoca  281  :  0.975\n",
            "Accuracy en la epoca  282  :  0.9833333333333333\n",
            "Accuracy en la epoca  283  :  0.975\n",
            "Accuracy en la epoca  284  :  0.9833333333333333\n",
            "Accuracy en la epoca  285  :  0.975\n",
            "Accuracy en la epoca  286  :  0.975\n",
            "Accuracy en la epoca  287  :  0.9833333333333333\n",
            "Accuracy en la epoca  288  :  0.975\n",
            "Accuracy en la epoca  289  :  0.975\n",
            "Accuracy en la epoca  290  :  0.975\n",
            "Accuracy en la epoca  291  :  0.975\n",
            "Accuracy en la epoca  292  :  0.975\n",
            "Accuracy en la epoca  293  :  0.9416666666666667\n",
            "Accuracy en la epoca  294  :  0.975\n",
            "Accuracy en la epoca  295  :  0.975\n",
            "Accuracy en la epoca  296  :  0.95\n",
            "Accuracy en la epoca  297  :  0.9833333333333333\n",
            "Accuracy en la epoca  298  :  0.975\n",
            "Accuracy en la epoca  299  :  0.975\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred"
      ],
      "metadata": {
        "id": "DZQAT5jGerps",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c22990c8-d418-4a1e-d9cd-2b2cc17653d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1.10531386e-03, 9.98894682e-01, 3.75408491e-09],\n",
              "       [9.79903331e-01, 2.00966694e-02, 5.92029825e-22],\n",
              "       [3.42013525e-07, 1.07232323e-02, 9.89276426e-01],\n",
              "       [9.99941869e-01, 5.81308037e-05, 9.36657082e-23],\n",
              "       [2.60871307e-09, 3.19123744e-05, 9.99968085e-01],\n",
              "       [9.96011219e-01, 3.98878139e-03, 1.69990354e-18],\n",
              "       [9.99973868e-01, 2.61323893e-05, 2.34718743e-22],\n",
              "       [9.99907780e-01, 9.22195877e-05, 3.79959339e-23],\n",
              "       [1.61163978e-12, 5.69876424e-08, 9.99999943e-01],\n",
              "       [1.08176544e-07, 9.84370607e-03, 9.90156186e-01],\n",
              "       [9.99890525e-01, 1.09475184e-04, 3.71920176e-22],\n",
              "       [9.99858160e-01, 1.41839769e-04, 9.62985294e-20],\n",
              "       [2.60591124e-04, 9.99735852e-01, 3.55640982e-06],\n",
              "       [9.99969769e-01, 3.02309473e-05, 3.03180962e-22],\n",
              "       [1.42490898e-09, 5.52192386e-05, 9.99944779e-01],\n",
              "       [7.27651067e-07, 2.35461646e-02, 9.76453108e-01],\n",
              "       [9.99998644e-01, 1.35569552e-06, 7.73485953e-24],\n",
              "       [9.99990455e-01, 9.54502363e-06, 2.66569540e-26],\n",
              "       [6.68256335e-09, 3.71648278e-05, 9.99962828e-01],\n",
              "       [8.98086356e-05, 9.96781993e-01, 3.12819805e-03],\n",
              "       [9.99975941e-01, 2.40591125e-05, 2.81820123e-23],\n",
              "       [9.99992755e-01, 7.24504108e-06, 1.76852753e-22],\n",
              "       [6.60757124e-04, 9.99173302e-01, 1.65940578e-04],\n",
              "       [1.49075981e-06, 8.64722741e-03, 9.91351282e-01],\n",
              "       [9.99970881e-01, 2.91192503e-05, 3.67203462e-24],\n",
              "       [3.25512718e-09, 4.71269926e-05, 9.99952870e-01],\n",
              "       [1.17581368e-05, 2.62911403e-02, 9.73697102e-01],\n",
              "       [1.54684998e-04, 9.98279856e-01, 1.56545885e-03],\n",
              "       [4.17329451e-08, 5.36039722e-04, 9.99463919e-01],\n",
              "       [8.34599260e-04, 9.99163209e-01, 2.19194561e-06],\n",
              "       [3.80540223e-05, 9.99769950e-01, 1.91995682e-04],\n",
              "       [3.70723993e-04, 9.96919329e-01, 2.70994730e-03],\n",
              "       [1.94064350e-05, 9.99980532e-01, 6.19305434e-08],\n",
              "       [9.99519996e-01, 4.80004092e-04, 2.03721570e-20],\n",
              "       [3.73854844e-05, 9.04389253e-01, 9.55733620e-02],\n",
              "       [1.90724443e-04, 9.99809265e-01, 1.06231786e-08],\n",
              "       [9.99966708e-01, 3.32915765e-05, 3.83928355e-24],\n",
              "       [9.99955204e-01, 4.47956467e-05, 9.11499793e-24],\n",
              "       [2.70665711e-05, 9.33465322e-01, 6.65076115e-02],\n",
              "       [1.01897302e-04, 9.99896673e-01, 1.42943628e-06],\n",
              "       [4.66758068e-04, 9.99330686e-01, 2.02556174e-04],\n",
              "       [3.42985927e-05, 4.05648532e-01, 5.94317169e-01],\n",
              "       [5.12954613e-08, 5.81104642e-04, 9.99418844e-01],\n",
              "       [2.36322079e-04, 9.99720700e-01, 4.29780188e-05],\n",
              "       [9.99502397e-01, 4.97603230e-04, 1.34900245e-21],\n",
              "       [1.76039847e-09, 4.17753105e-05, 9.99958223e-01],\n",
              "       [1.92493637e-08, 1.54892385e-03, 9.98451057e-01],\n",
              "       [8.24835256e-05, 9.99916644e-01, 8.72755879e-07],\n",
              "       [2.78577761e-04, 9.99052788e-01, 6.68634624e-04],\n",
              "       [1.12164104e-07, 7.36028790e-04, 9.99263859e-01],\n",
              "       [2.24629175e-07, 2.75411469e-03, 9.97245661e-01],\n",
              "       [9.99996017e-01, 3.98313705e-06, 1.63376005e-21],\n",
              "       [9.99491952e-01, 5.08047697e-04, 5.86075059e-22],\n",
              "       [9.83280347e-08, 9.67018343e-03, 9.90329718e-01],\n",
              "       [9.99975057e-01, 2.49427821e-05, 4.81856820e-22],\n",
              "       [1.08596150e-06, 3.47090269e-03, 9.96528011e-01],\n",
              "       [6.67799594e-05, 9.99621981e-01, 3.11238818e-04],\n",
              "       [3.06760539e-06, 7.24659200e-02, 9.27531012e-01],\n",
              "       [9.99993362e-01, 6.63816739e-06, 3.79459001e-24],\n",
              "       [9.99830197e-01, 1.69802551e-04, 8.77725177e-22],\n",
              "       [3.81969944e-05, 9.99961447e-01, 3.55989196e-07],\n",
              "       [1.65128266e-04, 9.99785760e-01, 4.91116963e-05],\n",
              "       [9.99947844e-01, 5.21560218e-05, 1.07415731e-22],\n",
              "       [3.66907483e-09, 1.69123547e-04, 9.99830873e-01],\n",
              "       [9.29967275e-07, 4.99409330e-01, 5.00589740e-01],\n",
              "       [9.99275761e-01, 7.24239240e-04, 1.35204142e-18],\n",
              "       [1.80874615e-11, 1.62176856e-06, 9.99998378e-01],\n",
              "       [2.25810745e-04, 9.99753062e-01, 2.11276273e-05],\n",
              "       [9.99998924e-01, 1.07586014e-06, 1.39877444e-23],\n",
              "       [3.82230980e-05, 5.51916564e-01, 4.48045213e-01],\n",
              "       [3.13141749e-05, 4.52192071e-01, 5.47776615e-01],\n",
              "       [5.17434138e-05, 6.80096158e-01, 3.19852099e-01],\n",
              "       [1.39913837e-07, 2.29189042e-03, 9.97707970e-01],\n",
              "       [1.42396776e-04, 9.99850344e-01, 7.25889097e-06],\n",
              "       [1.58361483e-07, 7.49406986e-04, 9.99250435e-01],\n",
              "       [1.75543785e-05, 3.01855747e-01, 6.98126699e-01],\n",
              "       [4.07061766e-06, 1.66741259e-01, 8.33254670e-01],\n",
              "       [2.18950880e-06, 2.39965053e-02, 9.76001305e-01],\n",
              "       [1.28214058e-05, 8.28027421e-02, 9.17184437e-01],\n",
              "       [9.97796811e-01, 2.20318934e-03, 4.81747529e-21],\n",
              "       [5.87099236e-04, 9.99412880e-01, 2.06454386e-08],\n",
              "       [8.36687250e-06, 4.92889892e-01, 5.07101741e-01],\n",
              "       [2.25743494e-05, 9.88521482e-01, 1.14559434e-02],\n",
              "       [2.59501591e-07, 1.45478779e-02, 9.85451863e-01],\n",
              "       [1.93762968e-08, 8.88721123e-05, 9.99911109e-01],\n",
              "       [9.98368521e-01, 1.63147939e-03, 9.77654079e-20],\n",
              "       [9.99797315e-01, 2.02685158e-04, 5.08517729e-20],\n",
              "       [3.32935524e-04, 9.94246871e-01, 5.42019337e-03],\n",
              "       [1.15336728e-04, 9.99884556e-01, 1.07211189e-07],\n",
              "       [9.99949239e-01, 5.07606389e-05, 2.90632565e-22],\n",
              "       [9.99934374e-01, 6.56259566e-05, 4.05637372e-21],\n",
              "       [9.99507916e-01, 4.92083874e-04, 6.40024877e-23],\n",
              "       [2.54998889e-05, 9.99815365e-01, 1.59135342e-04],\n",
              "       [1.49880070e-04, 9.99850110e-01, 1.02098369e-08],\n",
              "       [9.99703832e-01, 2.96168406e-04, 1.05267599e-22],\n",
              "       [9.99016258e-01, 9.83741953e-04, 2.35971275e-21],\n",
              "       [4.35000262e-06, 4.22538240e-02, 9.57741826e-01],\n",
              "       [1.45766780e-10, 1.28422494e-05, 9.99987158e-01],\n",
              "       [1.21965049e-04, 9.95911975e-01, 3.96606016e-03],\n",
              "       [1.19668551e-07, 2.95480203e-03, 9.97045078e-01],\n",
              "       [9.99298773e-01, 7.01227250e-04, 1.67436008e-20],\n",
              "       [2.48087154e-07, 2.48816592e-03, 9.97511586e-01],\n",
              "       [9.99341236e-01, 6.58764122e-04, 6.91862314e-22],\n",
              "       [2.74359509e-08, 6.51882567e-04, 9.99348090e-01],\n",
              "       [7.65012957e-05, 9.99802436e-01, 1.21062767e-04],\n",
              "       [9.99979761e-01, 2.02388980e-05, 4.27144196e-23],\n",
              "       [6.18187154e-04, 9.98903729e-01, 4.78084018e-04],\n",
              "       [2.78702995e-04, 9.99647429e-01, 7.38684940e-05],\n",
              "       [1.27712283e-09, 1.53873226e-05, 9.99984611e-01],\n",
              "       [9.99999732e-01, 2.68192015e-07, 7.50754643e-27],\n",
              "       [9.99777222e-01, 2.22778116e-04, 2.13418841e-23],\n",
              "       [9.99567376e-01, 4.32624183e-04, 1.53174565e-22],\n",
              "       [1.53732875e-04, 9.99845222e-01, 1.04505883e-06],\n",
              "       [3.49556604e-05, 4.75979895e-01, 5.23985149e-01],\n",
              "       [3.02199672e-04, 9.87175222e-01, 1.25225779e-02],\n",
              "       [4.72564934e-04, 9.98109945e-01, 1.41749019e-03],\n",
              "       [3.53661630e-04, 9.99594563e-01, 5.17755056e-05],\n",
              "       [9.99621395e-01, 3.78605413e-04, 2.86778065e-22],\n",
              "       [9.99903285e-01, 9.67154288e-05, 2.90109342e-22],\n",
              "       [9.06921555e-10, 4.03986702e-05, 9.99959600e-01]])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Metemos los datos de prueba en la red ya entrenada \n",
        "y_pred= pred(X_test,W1,b1,W2,b2)\n",
        "#Los devolvemos a sus etiquetas originales \n",
        "y_pred_ = np.argmax(y_pred, axis=1)\n",
        "y_test_ = np.argmax(y_test, axis=1)\n",
        "#Calculamos la precisión del modelo con los datos que reservamos que no fueron \n",
        "#usados para el entrenamiento\n",
        "print(\"Accuracy: \", accuracy_score(y_test_, y_pred_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uY4pQdfEI1_",
        "outputId": "8941721d-bddd-4404-cc23-e2fb3de51c6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy:  0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En conclusión...\n",
        "Notemos que el algoritmo propaga el error a través de las capas; desde la capa de salida hasta la capa de entrada, ajusta los pesos y los bias en función del error. \n",
        "\n",
        "Así, a diferencia de la regresión lógista, podemos hacer que la red aprenda tareas especificas, en este caso, con los datos utilizados podemos clasificar los distintos tipos de flores con las etiquetas dadas. Sin embargo, también es posible, con las modificaciones adecuadas, la predicción de valores numéricos en cualquier otro contexto. Y muchos otros ejemplos..."
      ],
      "metadata": {
        "id": "x284DcSjTAyR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5-ifORjfW3T4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}